{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import os, sys, re, glob, time, pickle, IPython, logging\n",
    "import scipy.ndimage as ndi\n",
    "from itertools import islice\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "#from torchmore import layers, flex\n",
    "#import torchtrainers as tt\n",
    "from torch.utils.data import DataLoader\n",
    "from webdataset import WebDataset, WebLoader\n",
    "from ocropus4train import ocrhelpers as helpers\n",
    "from ocropus4train.ocrhelpers import *\n",
    "from ocropus4train import ocrmodels2 as models\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import ocrodeg\n",
    "import imageio.v2 as imageio\n",
    "import braceexpand\n",
    "\n",
    "RUN(\"date\")\n",
    "RUN(\"hostname\")\n",
    "RUN(\"whoami\")\n",
    "RUN(\"nvidia-smi -L\")\n",
    "\n",
    "os.environ[\"GOPEN_VERBOSE\"] = \"0\"\n",
    "os.environ[\"WDS_VERBOSE_CACHE\"] = \"0\"\n",
    "if not \"CUDA_VISIBLE_DEVICES\" in os.environ:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_urls = list(braceexpand.braceexpand(\"gs://ocro-iaa/words/books-{000001..000653}-words.tar\"))\n",
    "training_urls += list(braceexpand.braceexpand(\"gs://ocro-iaa/lines/books-{000001..000653}-lines.tar\"))\n",
    "testing_urls = \"gs://ocro-iaa/words/books-000000-words.tar\"\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#training_urls = \"data/words-simple-training.tar\"\n",
    "#testing_urls = \"data/words-simple-testing.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(32, 127)]\n",
    "charset = DefaultCharset(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations:\n",
    "# - autocrop\n",
    "# - shift, scale\n",
    "# - threshold\n",
    "# - noise, offset, contrast\n",
    "\n",
    "from ocropus4train.ocraugment import maybe, aniso, distort, normalize, height_normalize, autoinvert, make_noise, threshold, noisify\n",
    "\n",
    "def random_padding(a, target):\n",
    "    if a.shape[0] < target-1:\n",
    "        # for smaller images, add random padding to the top\n",
    "        d = random.randint(0, int(target - a.shape[0])-1)\n",
    "        a = np.pad(a, ((d, 0), (0, 0)))\n",
    "    else:\n",
    "        # for larger images, add up to 10% random padding to the top\n",
    "        d = random.randint(0, int(a.shape[0] * 0.1)-1)\n",
    "        a = np.pad(a, ((d, 0), (0, 0)))\n",
    "    return a\n",
    "\n",
    "def preprocess(a, target=48.0):\n",
    "    assert isinstance(a, np.ndarray)\n",
    "    assert a.ndim == 2\n",
    "    a = normalize(a)\n",
    "    a = autoinvert(a)\n",
    "    a = random_padding(a, target)\n",
    "    if maybe(0.5):\n",
    "        a = noisify(a)\n",
    "    if maybe(0.5):\n",
    "        a = distort(a)\n",
    "    if maybe(0.5):\n",
    "        a = aniso(a)\n",
    "    if maybe(0.1):\n",
    "        sigma = 10**random.uniform(-0.3, 0.3)\n",
    "        a = scipy.ndimage.gaussian_filter(a, sigma)\n",
    "        a = normalize(a)\n",
    "    if maybe(0.1):\n",
    "        a = threshold(a)\n",
    "    if target is not None:\n",
    "        a = height_normalize(a, target*(1.0 + random.uniform(-0.2, 0.0)))\n",
    "    assert a.ndim == 2\n",
    "    return a\n",
    "\n",
    "\n",
    "figsize(24, 12)\n",
    "testimg = imageio.imread(\"samples/word.jpg\")\n",
    "for i in range(36):\n",
    "    subplot(6, 6, i+1)\n",
    "    imshow(preprocess(testimg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good(sample):\n",
    "    img, txt = sample\n",
    "    if img.shape[-1] < 10 or img.shape[-2] < 10 or img.shape[-1] > 2500 or img.shape[-2] > 150:\n",
    "        # print(\"bad image size\", img.shape)\n",
    "        return None\n",
    "    return img, txt\n",
    "\n",
    "def usm_image(img):\n",
    "    img = img - ndi.gaussian_filter(img, 16.0, mode=\"nearest\")\n",
    "    return img\n",
    "\n",
    "def img_tensor(img):\n",
    "    assert img.ndim == 2, img.shape\n",
    "    assert img.dtype == np.float32, img.dtype\n",
    "    assert np.amax(img) < 10.0  # make sure it already got normalied somewhere\n",
    "    return torch.tensor(img).unsqueeze(0)\n",
    "\n",
    "def str_tensor(s):\n",
    "    assert isinstance(s, str)\n",
    "    return torch.tensor(charset.encode(s)).long()\n",
    "\n",
    "def pipeline(ds):\n",
    "    return ds.decode(\"l8\").to_tuple(\"jpg;jpeg;ppm;png txt\").map(good).map_tuple(preprocess).map_tuple(usm_image).map_tuple(img_tensor, str_tensor)\n",
    "\n",
    "training = pipeline(WebDataset(training_urls, resampled=True).shuffle(20000))\n",
    "testing = pipeline(WebDataset(testing_urls))\n",
    "training_dl = WebLoader(training, batch_size=batch_size, collate_fn=helpers.collate4trans, num_workers=8).with_epoch(100000//batch_size)\n",
    "testing_dl = WebLoader(testing, batch_size=batch_size, collate_fn=helpers.collate4trans, num_workers=4)\n",
    "images, sequences = next(iter(training_dl))\n",
    "assert images.shape[-2] <= 48, images.shape\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ocropus4train import ocrmodels2\n",
    "from importlib import reload\n",
    "reload(ocrmodels2)\n",
    "mname = \"tf_v1\"\n",
    "model = ocrmodels2.make(mname, noutput=len(charset))\n",
    "# ensure it can be jitted\n",
    "torch.jit.script(model);\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "errors = [(-1, time.time())]\n",
    "\n",
    "\n",
    "for trial in range(10):\n",
    "    model = models.make(mname, noutput=len(charset))\n",
    "    trainer = helpers.LineTrainer(model, charset=charset, lr=1e-5, mode=\"tf\")\n",
    "    trainer.save_jit = False  # FIXME\n",
    "    trainer.load_best()\n",
    "    try:\n",
    "        trainer.train(training_dl, 10, every=15, learning_rates=[1e-4]*3 + [1e-5]*200)\n",
    "    except helpers.NanError:\n",
    "        errors += [(trial, time.time())]\n",
    "        print(\"NaN Error during training, restarting and reloading from last checkpoint\")\n",
    "        time.sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
